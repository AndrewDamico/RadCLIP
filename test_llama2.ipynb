{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"clip_classes.py: Contains the CLIPDataset and CLIPTrainer wrapper\"\"\"\n",
    "\n",
    "__author__ = \"Christoper Alexander\"\n",
    "__copyright__ = \"Copyright 2023\"\n",
    "__credits__ = [\"Andrew D'Amico\", \"Christoper Alexander\", \"Katya Nosulko\", \"Vivek Chamala\", \"Matthew Conger\"]\n",
    "__license__ = \"\"\n",
    "__version__ = \"0.0.1\"\n",
    "__maintainer__ = \"Andrew Damico\"\n",
    "__email__ = \"andrew.damico@u.northwestern.edu\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82fd6dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/transformers/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ubuntu/miniconda3/envs/transformers/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda116.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary /home/ubuntu/miniconda3/envs/transformers/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda116.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/transformers/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/ubuntu/miniconda3 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    ")\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28bce8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"elinas/llama-7b-hf-transformers-4.29\")\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75c0f18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:38<00:00, 19.01s/it]\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"elinas/llama-7b-hf-transformers-4.29\",\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dba85d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_int8_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "558161dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "816b5d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5439f345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4194304 || all params: 6742609920 || trainable%: 0.06220594176090199\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbbab2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/ubuntu/.cache/huggingface/datasets/text/default-d2a13576f1036f22/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 8380.23it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 1429.06it/s]\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/text/default-d2a13576f1036f22/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 25.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets from the text files\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"train.txt\", \"test\": \"test.txt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "381f33f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    lambda examples: tokenizer(examples[\"text\"], max_length=2048, truncation=True, padding=\"max_length\"), batched=True\n",
    ")\n",
    "test_dataset = dataset[\"test\"].map(\n",
    "    lambda examples: tokenizer(examples[\"text\"], max_length=2048, truncation=True, padding=\"max_length\"), batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e12ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9496c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"test_llama_7b_2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    warmup_steps=50,\n",
    "    prediction_loss_only=True,\n",
    "    logging_dir=\"logs\",\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    half_precision_backend=\"auto\",  # Set the backend for mixed precision training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1b07b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a8922b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65530bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_state_dict = model.state_dict\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(\n",
    "        self, old_state_dict()\n",
    "    )\n",
    ").__get__(model, type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1f6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/transformers/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='75960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   67/75960 14:07 < 274:52:18, 0.08 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23d7dbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 12477\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13320386",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"llama_7b_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "667e8a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c8d12aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = generator(\"Tumor discovered\", max_length=300, num_return_sequences=2, do_sample=True, num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01aa543a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tumor discovered on the surface of the oesophagus. The patient is a 60-year-old man who complains of a 2-year history of a progressive dysphagia. The patient underwent an endoscopic examination, which revealed a dysplastic lesion in the oesophagus. The patient underwent an oesophageal resection. The pathological examination revealed an oesophageal adenocarcinoma with a high-grade dysplasia. The patient was treated with adjuvant chemotherapy and radiotherapy. The prognosis of oesophageal adenocarcinoma is poor due to the fact that it is usually diagnosed at a late stage. The 5-year survival rate is only 10-20%. Oesophageal adenocarcinoma is a neoplasm of the oesophageal mucosa. It is the most common type of oesophageal cancer, accounting for 50-70% of all oesophageal cancers. Oesophageal adenocarcinoma is most commonly diagnosed in the 6th and 7th decades of life. The most common symptoms of oesophageal adenocarcinoma are dysphagia\n"
     ]
    }
   ],
   "source": [
    "print(test[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9237c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tumor discovered in the liver. It was 1.5 cm in size. The liver was removed and sent to the pathology department. The pathology report showed metastasis of the colon adenocarcinoma to the liver. The patient was diagnosed with metastatic colon adenocarcinoma to the liver. The patient was treated with palliative chemotherapy. The patient died 2 months after the diagnosis of liver metastasis. Colorectal cancer is one of the most common cancers in the world. It is the third most common cause of cancer-related deaths. Colorectal cancer is one of the most common cancers in the world. It is the third most common cause of cancer-related deaths. Colorectal cancer is one of the most common cancers in the world. It is the third most common cause of cancer-related deaths. Colorectal cancer is one of the most common cancers in the world. It is the third most common cause of cancer-related deaths. Colorectal cancer is one of the most common cancers in the world. It is the third most common cause of cancer-related deaths. Colorectal cancer is one of the most common cancers in the world. It is the third most common cause of cancer-related deaths. Colorectal cancer is one of the most\n"
     ]
    }
   ],
   "source": [
    "print(test[1][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2af317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (transformers)",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
