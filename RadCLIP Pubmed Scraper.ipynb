{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cecc488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"RadCLIP Pubmed Scraper\"\"\"\n",
    "\n",
    "__author__ = \"Christoper Alexander\"\n",
    "__copyright__ = \"Copyright 2023\"\n",
    "__credits__ = [\"Andrew D'Amico\", \"Christoper Alexander\", \"Katya Nosulko\", \"Vivek Chamala\", \"Matthew Conger\"]\n",
    "__license__ = \"\"\n",
    "__version__ = \"0.0.1\"\n",
    "__maintainer__ = \"Andrew Damico\"\n",
    "__email__ = \"andrew.damico@u.northwestern.edu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9343d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from Bio import Entrez\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea08597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_tags_from_string(my_string: str) -> str:\n",
    "    soup = BeautifulSoup(my_string, 'html.parser')\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1856c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags_and_content(text):\n",
    "    # This regular expression pattern will match both opening and closing HTML/XML tags,\n",
    "    # as well as their content (including nested tags).\n",
    "    pattern = r'<[^>]*>[^<]*</[^>]*>|<[^/>]+/>'\n",
    "\n",
    "    # Use the re.sub function to replace all matches of the pattern with an empty string.\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "900f6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags_and_content2(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # Find all tags in the soup\n",
    "    for tag in soup.find_all(True):\n",
    "        # Extract the tag's parent and replace the tag with an empty string\n",
    "        tag.extract()\n",
    "\n",
    "    # Get the cleaned text\n",
    "    cleaned_text = soup.get_text()\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d506f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pattern = r\"http\\S+\"\n",
    "url_regexp = re.compile(url_pattern)\n",
    "\n",
    "\n",
    "def replace_urls_in_string(my_string: str) -> str:\n",
    "    # Replace a URL with the string \"URL\" \n",
    "    return url_regexp.sub(\"URL\", my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67d9c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_chars(text):\n",
    "    # Replace newline characters and carriage returns with spaces\n",
    "    text = re.sub(r'[\\n\\r]', ' ', text)\n",
    "\n",
    "    # Remove non-standard UTF elements (e.g., \"\\xa0\", \"\\u2009\") by replacing them with a space\n",
    "    text = re.sub(r'[\\u00A0\\u2009\\xa0]', ' ', text)\n",
    "\n",
    "    # Replace multiple consecutive spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c98c4418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_corrupt_utf_from_string(my_string: str) -> str:\n",
    "    return ''.join([c if unicodedata.is_normalized('NFC', c) else ' ' for c in my_string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d70f9fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_pattern = r'[-.,:;,!?\\'\"\\[\\]\\(\\)\\{\\}|\\\\`#\\$%@^&*_+<>“”—’‘/]+'\n",
    "punctuation_regexp = re.compile(punctuation_pattern)\n",
    "\n",
    "\n",
    "def strip_punctuation_from_string(my_string: str) -> str:\n",
    "    return punctuation_regexp.sub(\"\", my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "940ec169",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_pattern = r'\\d+'\n",
    "numbers_regexp = re.compile(numbers_pattern)\n",
    "\n",
    "\n",
    "def strip_numbers_from_string(my_string: str) -> str:\n",
    "    return numbers_regexp.sub(\"\", my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1719c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_pattern = '(\\s+)(a|an|the)(\\s+)'\n",
    "articles_regexp = re.compile(articles_pattern, re.IGNORECASE)\n",
    "\n",
    "\n",
    "def strip_articles_from_string(my_string: str) -> str:\n",
    "    return articles_regexp.sub(\" \", my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52c9eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepositions_pattern = '(\\s+)(about|above|across|after|against|along|among|around|at|away|before|behind|below|between|by|during|for|from|in|into|like|out|since|than|through|to|toward|under|until|upon|with|within|without)(\\s+)'\n",
    "prepositions_regexp = re.compile(prepositions_pattern, re.IGNORECASE)\n",
    "\n",
    "\n",
    "def strip_prepositions_from_string(my_string: str) -> str:\n",
    "    return prepositions_regexp.sub(\" \", my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfc1f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(document: str) -> str:\n",
    "    doc = remove_tags_and_content2(document)\n",
    "    doc = doc.strip()\n",
    "    doc = remove_special_chars(doc)\n",
    "    doc = replace_urls_in_string(doc)\n",
    "    doc = strip_corrupt_utf_from_string(doc)\n",
    "    doc = strip_punctuation_from_string(doc)\n",
    "    doc = strip_numbers_from_string(doc)\n",
    "    doc = strip_articles_from_string(doc)\n",
    "    doc = strip_prepositions_from_string(doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dda15504",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_QUERY = 'medline[sb] AND \"open access\"[filter]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a1dbbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, db=\"pmc\"):\n",
    "    Entrez.email = \"christopheralexander2023@u.northwestern.edu\"\n",
    "    new_query = f\"{query} AND {BASE_QUERY}\"\n",
    "    handle = Entrez.esearch(db=db,\n",
    "                            sort=\"relevance\",\n",
    "                            retmax=\"20\",\n",
    "                            retmode=\"xml\",\n",
    "                            term=new_query,\n",
    "                            usehistory=\"y\")\n",
    "    results = Entrez.read(handle)\n",
    "    return results\n",
    "\n",
    "\n",
    "def fetch_details(results):\n",
    "    ids = ','.join(results[\"IdList\"])\n",
    "    Entrez.email = \"christopheralexander2023@u.northwestern.edu\"\n",
    "    handle = Entrez.efetch(db=\"pmc\",\n",
    "                           rettype=\"full\",\n",
    "                           retmode=\"xml\",\n",
    "                           id=ids,\n",
    "                           webenv=results[\"WebEnv\"],\n",
    "                           query_key=results[\"QueryKey\"])\n",
    "    docs = Entrez.read(handle)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e452ab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_section(section):\n",
    "    if not section.get(\"sec\", []):\n",
    "        return [section]\n",
    "\n",
    "    result = []\n",
    "    subs = section[\"sec\"]\n",
    "    if isinstance(subs, list):\n",
    "        for sub in section[\"sec\"]:\n",
    "            result.extend(process_section(sub))\n",
    "    else:\n",
    "        return [subs]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d643835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "    results = search(query)\n",
    "    docs = fetch_details(results)\n",
    "\n",
    "    processed_docs = []\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_dict = {}\n",
    "        counter = 0\n",
    "        for section in process_section(doc[\"body\"]):\n",
    "            title = section.get(\"title\", \"no_title\")\n",
    "            if not title:\n",
    "                title = \"no_title\"\n",
    "            sec_id = section.attributes.get(\"id\", \"no_id\")\n",
    "            string_elements = section[\"p\"]\n",
    "            if isinstance(string_elements, list):\n",
    "                for string_element in section[\"p\"]:\n",
    "                    text = str(string_element)\n",
    "                    sub_id = string_element.attributes.get(\"id\", \"no_sub_id\")\n",
    "                    doc_dict[(title, sec_id, sub_id, counter)] = clean_doc(text)\n",
    "                    counter += 1\n",
    "        processed_docs.append(doc_dict)\n",
    "        print(f\"processed {i + 1} docs\")\n",
    "\n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e2d21ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 1 docs\n",
      "processed 2 docs\n",
      "processed 3 docs\n",
      "processed 4 docs\n",
      "processed 5 docs\n",
      "processed 6 docs\n",
      "processed 7 docs\n",
      "processed 8 docs\n",
      "processed 9 docs\n",
      "processed 10 docs\n",
      "processed 11 docs\n",
      "processed 12 docs\n",
      "processed 13 docs\n",
      "processed 14 docs\n",
      "processed 15 docs\n",
      "processed 16 docs\n",
      "processed 17 docs\n",
      "processed 18 docs\n",
      "processed 19 docs\n",
      "processed 20 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/biomed/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test = process_query(\"cancer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dcd90d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search(\"cancer\")\n",
    "docs = fetch_details(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "731e634b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sec': DictElement({'title': {}, 'p': DictElement({}, attributes={'id': 'para430'})}, attributes={'id': 'cesec80'})}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0][\"body\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec2aaae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (biomed)",
   "language": "python",
   "name": "biomed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
