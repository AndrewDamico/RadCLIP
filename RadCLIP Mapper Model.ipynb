{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88037c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "'''\n",
    "RadCLIP Mapper Model Training Code\n",
    "'''\n",
    "\n",
    "__author__ = \"Christoper Alexander\"\n",
    "__copyright__ = \"Copyright 2023\"\n",
    "__credits__ = [\"Andrew D'Amico\", \"Christoper Alexander\", \"Katya Nosulko\", \"Vivek Chamala\", \"Matthew Conger\"]\n",
    "__license__ = \"\"\n",
    "__version__ = \"0.0.1\"\n",
    "__maintainer__ = \"Andrew Damico\"\n",
    "__email__ = \"andrew.damico@u.northwestern.edu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1abd2bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/transformers/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from enum import Enum\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as nnf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90647fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CHECKPOINT = \"gptmedium_10k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8302bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappingType(Enum):\n",
    "    MLP = 'mlp'\n",
    "    Transformer = 'transformer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c061953",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipCocoDataset(Dataset):\n",
    "    def __init__(self, data_path: str, prefix_length: int, gpt2_type: str = GPT_CHECKPOINT,\n",
    "                 normalize_prefix=False):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.prefix_length = prefix_length\n",
    "        self.normalize_prefix = normalize_prefix\n",
    "        with open(data_path, 'rb') as f:\n",
    "            all_data = pickle.load(f)\n",
    "        print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n",
    "        sys.stdout.flush()\n",
    "        self.prefixes = all_data[\"clip_embedding\"]\n",
    "        captions_raw = all_data[\"captions\"]\n",
    "        # self.image_ids = [caption[\"image_id\"] for caption in captions_raw]\n",
    "        self.captions = [caption['caption'] for caption in captions_raw]\n",
    "\n",
    "        self.captions_tokens = []\n",
    "        self.caption2embedding = []\n",
    "        max_seq_len = 0\n",
    "        for caption in captions_raw:\n",
    "            self.captions_tokens.append(\n",
    "                torch.tensor(self.tokenizer.encode(caption['caption']), dtype=torch.int64)\n",
    "            )\n",
    "            self.caption2embedding.append(caption[\"clip_embedding\"])\n",
    "            max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.captions_tokens)\n",
    "\n",
    "    def pad_tokens(self, item: int):\n",
    "        tokens = self.captions_tokens[item]\n",
    "        padding = self.max_seq_len - tokens.shape[0]\n",
    "        if padding > 0:\n",
    "            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n",
    "            self.captions_tokens[item] = tokens\n",
    "        elif padding < 0:\n",
    "            tokens = tokens[:self.max_seq_len]\n",
    "            self.captions_tokens[item] = tokens\n",
    "        mask = tokens.ge(0)  # mask is zero where we out of sequence\n",
    "        tokens[~mask] = 0\n",
    "        mask = mask.float()\n",
    "        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # adding prefix mask\n",
    "        return tokens, mask\n",
    "\n",
    "    def __getitem__(self, item: int) -> Tuple[torch.Tensor, ...]:\n",
    "        tokens, mask = self.pad_tokens(item)\n",
    "        prefix = self.prefixes[self.caption2embedding[item]]\n",
    "        if self.normalize_prefix:\n",
    "            prefix = prefix.float()\n",
    "            prefix = prefix / prefix.norm(2, -1)\n",
    "        return tokens, mask, prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc7b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(act())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class MlpTransformer(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, out_d: Optional[int] = None, act=nnf.relu, dropout=0.):\n",
    "        super().__init__()\n",
    "        out_d = out_d if out_d is not None else in_dim\n",
    "        self.fc1 = nn.Linear(in_dim, h_dim)\n",
    "        self.act = act\n",
    "        self.fc2 = nn.Linear(h_dim, out_d)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6cbe189",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_self, dim_ref, num_heads, bias=True, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim_self // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.to_queries = nn.Linear(dim_self, dim_self, bias=bias)\n",
    "        self.to_keys_values = nn.Linear(dim_ref, dim_self * 2, bias=bias)\n",
    "        self.project = nn.Linear(dim_self, dim_self)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        y = y if y is not None else x\n",
    "        b, n, c = x.shape\n",
    "        _, m, d = y.shape\n",
    "        # b n h dh\n",
    "        queries = self.to_queries(x).reshape(b, n, self.num_heads, c // self.num_heads)\n",
    "        # b m 2 h dh\n",
    "        keys_values = self.to_keys_values(y).reshape(b, m, 2, self.num_heads, c // self.num_heads)\n",
    "        keys, values = keys_values[:, :, 0], keys_values[:, :, 1]\n",
    "        attention = torch.einsum('bnhd,bmhd->bnmh', queries, keys) * self.scale\n",
    "        if mask is not None:\n",
    "            if mask.dim() == 2:\n",
    "                mask = mask.unsqueeze(1)\n",
    "            attention = attention.masked_fill(mask.unsqueeze(3), float(\"-inf\"))\n",
    "        attention = attention.softmax(dim=2)\n",
    "        out = torch.einsum('bnmh,bmhd->bnhd', attention, values).reshape(b, n, c)\n",
    "        out = self.project(out)\n",
    "        return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b72c5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, dim_self, dim_ref, num_heads, mlp_ratio=4., bias=False, dropout=0., act=nnf.relu,\n",
    "                 norm_layer: nn.Module = nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim_self)\n",
    "        self.attn = MultiHeadAttention(dim_self, dim_ref, num_heads, bias=bias, dropout=dropout)\n",
    "        self.norm2 = norm_layer(dim_self)\n",
    "        self.mlp = MlpTransformer(dim_self, int(dim_self * mlp_ratio), act=act, dropout=dropout)\n",
    "\n",
    "    def forward_with_attention(self, x, y=None, mask=None):\n",
    "        x_, attention = self.attn(self.norm1(x), y, mask)\n",
    "        x = x + x_\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x, attention\n",
    "\n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        x = x + self.attn(self.norm1(x), y, mask)[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c791be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim_self: int, num_heads: int, num_layers: int, dim_ref: Optional[int] = None,\n",
    "                 mlp_ratio: float = 2., act=nnf.relu, norm_layer: nn.Module = nn.LayerNorm, enc_dec: bool = False):\n",
    "        super(Transformer, self).__init__()\n",
    "        dim_ref = dim_ref if dim_ref is not None else dim_self\n",
    "        self.enc_dec = enc_dec\n",
    "        if enc_dec:\n",
    "            num_layers = num_layers * 2\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            if i % 2 == 0 and enc_dec:  # cross\n",
    "                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
    "            elif enc_dec:  # self\n",
    "                layers.append(\n",
    "                    TransformerLayer(dim_self, dim_self, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
    "            else:  # self or cross\n",
    "                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward_with_attention(self, x, y=None, mask=None):\n",
    "        attentions = []\n",
    "        for layer in self.layers:\n",
    "            x, att = layer.forward_with_attention(x, y, mask)\n",
    "            attentions.append(att)\n",
    "        return x, attentions\n",
    "\n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i % 2 == 0 and self.enc_dec:  # cross\n",
    "                x = layer(x, y)\n",
    "            elif self.enc_dec:  # self\n",
    "                x = layer(x, x, mask)\n",
    "            else:  # self or cross\n",
    "                x = layer(x, y, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c0d0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerMapper(nn.Module):\n",
    "    def __init__(self, dim_clip: int, dim_embedding: int, prefix_length: int, clip_length: int, num_layers: int = 8):\n",
    "        super(TransformerMapper, self).__init__()\n",
    "        self.clip_length = clip_length\n",
    "        self.transformer = Transformer(dim_embedding, 8, num_layers)\n",
    "        self.linear = nn.Linear(dim_clip, clip_length * dim_embedding)\n",
    "        self.prefix_const = nn.Parameter(torch.randn(prefix_length, dim_embedding), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x).view(x.shape[0], self.clip_length, -1)\n",
    "        prefix = self.prefix_const.unsqueeze(0).expand(x.shape[0], *self.prefix_const.shape)\n",
    "        prefix = torch.cat((x, prefix), dim=1)\n",
    "        out = self.transformer(prefix)[:, self.clip_length:]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59f800c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipCaptionModel(nn.Module):\n",
    "    def __init__(self, prefix_length: int, clip_length: Optional[int] = None, prefix_size: int = 512,\n",
    "                 num_layers: int = 8, mapping_type: MappingType = MappingType.MLP):\n",
    "        super(ClipCaptionModel, self).__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained(GPT_CHECKPOINT)\n",
    "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "        if mapping_type == MappingType.MLP:\n",
    "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n",
    "                                     self.gpt_embedding_size * prefix_length))\n",
    "        else:\n",
    "            self.clip_project = TransformerMapper(prefix_size, self.gpt_embedding_size, prefix_length,\n",
    "                                                  clip_length, num_layers)\n",
    "\n",
    "    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
    "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,\n",
    "                labels: Optional[torch.Tensor] = None):\n",
    "        embedding_text = self.gpt.transformer.wte(tokens)\n",
    "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
    "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
    "        if labels is not None:\n",
    "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
    "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
    "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5060947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        train_dataset: ClipCocoDataset,\n",
    "        eval_dataset: ClipCocoDataset,\n",
    "        model: ClipCaptionModel,\n",
    "        batch_size,\n",
    "        epochs,\n",
    "        lr: float = 2e-5,\n",
    "        warmup_steps: int = 5000,\n",
    "        output_dir: str = \".\",\n",
    "        output_prefix: str = \"\",\n",
    "        accum_iter: int = 4\n",
    "):\n",
    "    device = torch.device('cuda')\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader)\n",
    "    )\n",
    "    all_train_losses = {}\n",
    "    all_eval_losses = {}\n",
    "    for epoch in range(epochs):\n",
    "        print(f\">>> Training epoch {epoch}\")\n",
    "        model.train()\n",
    "        sys.stdout.flush()\n",
    "        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n",
    "        train_losses = []\n",
    "        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n",
    "            model.zero_grad()\n",
    "            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n",
    "            outputs = model(tokens, prefix, mask)\n",
    "            logits = outputs.logits[:, train_dataset.prefix_length - 1: -1]\n",
    "            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n",
    "            loss.backward()\n",
    "\n",
    "            # weights update\n",
    "            # if ((idx + 1) % accum_iter == 0) or (idx + 1 == len(train_dataloader)):\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss = loss.item()\n",
    "            progress.set_postfix({\"loss\": train_loss})\n",
    "            train_losses.append(train_loss)\n",
    "            progress.update()\n",
    "            if (idx + 1) % 10000 == 0:\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(output_dir, f\"{output_prefix}_latest.pt\"),\n",
    "                )\n",
    "        progress.close()\n",
    "        # print(f\"Total train loss for epoch {epoch} is {accum_train_loss}\")\n",
    "        if epoch % 1 == 0 or epoch == epochs - 1:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}.pt\"),\n",
    "            )\n",
    "\n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        sys.stdout.flush()\n",
    "        progress = tqdm(total=len(eval_dataloader), desc=output_prefix)\n",
    "        eval_losses = []\n",
    "        for idx, (tokens, mask, prefix) in enumerate(eval_dataloader):\n",
    "            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n",
    "            outputs = model(tokens, prefix, mask)\n",
    "            logits = outputs.logits[:, eval_dataset.prefix_length - 1: -1]\n",
    "            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n",
    "            eval_loss = loss.item()\n",
    "            progress.set_postfix({\"loss\": eval_loss})\n",
    "            eval_losses.append(eval_loss)\n",
    "            progress.update()\n",
    "\n",
    "        progress.close()\n",
    "        all_train_losses[f\"epoch_{epoch}\"] = train_losses.copy()\n",
    "        all_eval_losses[f\"epoch_{epoch}\"] = eval_losses.copy()\n",
    "        # print(f\"Total eval loss for epoch {epoch} is {accum_eval_loss}\")\n",
    "\n",
    "    return model, all_train_losses, all_eval_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f355ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size is 71866\n"
     ]
    }
   ],
   "source": [
    "prefix_length = 10\n",
    "train_dataset = ClipCocoDataset(\n",
    "    \"radclip_transformer_train_new.p\",\n",
    "    prefix_length,\n",
    "    gpt2_type=GPT_CHECKPOINT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db886e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size is 3783\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = ClipCocoDataset(\n",
    "    \"radclip_transformer_test_new.p\",\n",
    "    prefix_length,\n",
    "    gpt2_type=GPT_CHECKPOINT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06534316",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a0d532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClipCaptionModel(\n",
    "    prefix_length, clip_length=10, prefix_size=prefix_dim, num_layers=8, mapping_type=MappingType.Transformer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca8633d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85aca3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/transformers/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "new_radclip_gptmed:  44%|████▍     | 13451/30697 [1:47:57<2:18:15,  2.08it/s, loss=1.13]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "new_radclip_gptmed:  69%|██████▊   | 21104/30697 [2:49:39<1:17:06,  2.07it/s, loss=1.27] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "new_radclip_gptmed: 100%|██████████| 30697/30697 [4:06:54<00:00,  2.07it/s, loss=1.44] \n",
      "new_radclip_gptmed: 100%|██████████| 1621/1621 [02:13<00:00, 12.10it/s, loss=1.47] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "new_radclip_gptmed:  18%|█▊        | 5444/30697 [43:34<3:21:58,  2.08it/s, loss=1.32] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "new_radclip_gptmed:  95%|█████████▍| 29109/30697 [3:53:56<12:44,  2.08it/s, loss=1.45]   IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "new_radclip_gptmed:  15%|█▌        | 4750/30697 [38:02<3:27:28,  2.08it/s, loss=1.14] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "new_radclip_gptmed:  41%|████      | 12504/30697 [1:40:21<2:25:30,  2.08it/s, loss=1.45] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "new_radclip_gptmed:  65%|██████▌   | 19964/30697 [2:40:07<1:25:58,  2.08it/s, loss=1.22] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "new_radclip_gptmed:  90%|█████████ | 27670/30697 [3:42:09<24:15,  2.08it/s, loss=1.46] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "new_radclip_gptmed:  11%|█         | 3268/30697 [26:10<3:39:45,  2.08it/s, loss=1.37] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "new_radclip_gptmed:  34%|███▍      | 10406/30697 [1:23:34<2:42:18,  2.08it/s, loss=1.65] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "new_radclip_gptmed:  58%|█████▊    | 17884/30697 [2:23:31<1:42:38,  2.08it/s, loss=1.31] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "new_radclip_gptmed:  83%|████████▎ | 25433/30697 [3:24:14<42:06,  2.08it/s, loss=1.42] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "new_radclip_gptmed: 100%|██████████| 1621/1621 [02:13<00:00, 12.13it/s, loss=1.4]  \n"
     ]
    }
   ],
   "source": [
    "model, tloss, eloss = train(train_dataset, eval_dataset, model, 14, 4, output_prefix=\"new_radclip_gptmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83f5fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"clip_caption_model_transformer_10k_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82aa66f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4610324561559147"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(eloss[\"epoch_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69411007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4803535490033068"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(eloss[\"epoch_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1ddd6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3629426956176758,\n",
       " 1.2069710493087769,\n",
       " 1.8457428216934204,\n",
       " 1.8556292057037354,\n",
       " 1.4643269777297974,\n",
       " 1.043078899383545,\n",
       " 1.5649605989456177,\n",
       " 1.338426947593689,\n",
       " 1.1791372299194336,\n",
       " 1.363163709640503,\n",
       " 1.669366478919983,\n",
       " 1.3977620601654053,\n",
       " 1.6598552465438843,\n",
       " 1.3277424573898315,\n",
       " 1.4018137454986572,\n",
       " 1.5364246368408203,\n",
       " 1.4133042097091675,\n",
       " 1.5706279277801514,\n",
       " 1.4096860885620117,\n",
       " 1.4590132236480713,\n",
       " 1.4180641174316406,\n",
       " 1.3646734952926636,\n",
       " 1.461068868637085,\n",
       " 1.094805121421814,\n",
       " 1.3705743551254272,\n",
       " 1.1924760341644287,\n",
       " 1.5594137907028198,\n",
       " 1.4929217100143433,\n",
       " 1.0139890909194946,\n",
       " 1.6037801504135132,\n",
       " 1.9003727436065674,\n",
       " 1.5881577730178833,\n",
       " 1.5539162158966064,\n",
       " 1.5637794733047485,\n",
       " 1.5678178071975708,\n",
       " 1.6485785245895386,\n",
       " 1.388034701347351,\n",
       " 1.754345178604126,\n",
       " 1.1583211421966553,\n",
       " 1.677202582359314,\n",
       " 1.9811991453170776,\n",
       " 1.3265657424926758,\n",
       " 1.5462720394134521,\n",
       " 1.866016149520874,\n",
       " 1.4394484758377075,\n",
       " 1.4259629249572754,\n",
       " 1.3053988218307495,\n",
       " 1.4140522480010986,\n",
       " 1.8304939270019531,\n",
       " 1.3987809419631958,\n",
       " 1.446761965751648,\n",
       " 1.2644412517547607,\n",
       " 1.434773325920105,\n",
       " 1.6427019834518433,\n",
       " 1.15041184425354,\n",
       " 1.2782617807388306,\n",
       " 1.4823132753372192,\n",
       " 1.540889024734497,\n",
       " 1.572229266166687,\n",
       " 1.283991813659668,\n",
       " 1.5198663473129272,\n",
       " 1.658704400062561,\n",
       " 1.3284401893615723,\n",
       " 1.1940217018127441,\n",
       " 1.546122670173645,\n",
       " 1.427625060081482,\n",
       " 1.487758755683899,\n",
       " 1.4498324394226074,\n",
       " 1.779244065284729,\n",
       " 1.2570228576660156,\n",
       " 1.318833589553833,\n",
       " 1.7087736129760742,\n",
       " 1.2131626605987549,\n",
       " 1.6398489475250244,\n",
       " 1.664522409439087,\n",
       " 1.2920019626617432,\n",
       " 1.5282230377197266,\n",
       " 1.3296687602996826,\n",
       " 1.9396958351135254,\n",
       " 1.5951786041259766,\n",
       " 1.3099199533462524,\n",
       " 1.1837793588638306,\n",
       " 1.4143692255020142,\n",
       " 1.4033645391464233,\n",
       " 1.4887772798538208,\n",
       " 1.5142126083374023,\n",
       " 1.6124804019927979,\n",
       " 1.2408024072647095,\n",
       " 1.605563759803772,\n",
       " 1.372297763824463,\n",
       " 1.7233710289001465,\n",
       " 1.543583631515503,\n",
       " 1.301025390625,\n",
       " 1.2405935525894165,\n",
       " 1.37747323513031,\n",
       " 1.3857876062393188,\n",
       " 1.3937594890594482,\n",
       " 1.2859033346176147,\n",
       " 1.1420120000839233,\n",
       " 1.8703405857086182,\n",
       " 1.4118491411209106,\n",
       " 1.4459656476974487,\n",
       " 1.3440313339233398,\n",
       " 1.56864595413208,\n",
       " 1.5544573068618774,\n",
       " 1.4906431436538696,\n",
       " 1.321839690208435,\n",
       " 1.3402519226074219,\n",
       " 1.2568321228027344,\n",
       " 1.430173635482788,\n",
       " 1.2931568622589111,\n",
       " 1.4571031332015991,\n",
       " 1.6733347177505493,\n",
       " 1.1767995357513428,\n",
       " 1.2113350629806519,\n",
       " 1.2927583456039429,\n",
       " 1.407467007637024,\n",
       " 1.5324510335922241,\n",
       " 1.1323375701904297,\n",
       " 1.5769407749176025,\n",
       " 1.7571457624435425,\n",
       " 1.5828040838241577,\n",
       " 1.4767932891845703,\n",
       " 1.5982418060302734,\n",
       " 1.9120533466339111,\n",
       " 1.4446464776992798,\n",
       " 1.3749326467514038,\n",
       " 1.14173424243927,\n",
       " 1.2166097164154053,\n",
       " 1.2839478254318237,\n",
       " 1.484675407409668,\n",
       " 1.2051060199737549,\n",
       " 1.1950757503509521,\n",
       " 1.2478610277175903,\n",
       " 1.7774591445922852,\n",
       " 2.0402488708496094,\n",
       " 1.3824334144592285,\n",
       " 1.3758809566497803,\n",
       " 1.441868543624878,\n",
       " 1.3016210794448853,\n",
       " 1.6157386302947998,\n",
       " 1.5384314060211182,\n",
       " 1.3460506200790405,\n",
       " 1.3163758516311646,\n",
       " 1.472110390663147,\n",
       " 1.560123085975647,\n",
       " 1.7678712606430054,\n",
       " 1.6687941551208496,\n",
       " 1.3709557056427002,\n",
       " 1.2903333902359009,\n",
       " 1.2400389909744263,\n",
       " 1.4171112775802612,\n",
       " 1.7150646448135376,\n",
       " 1.5816482305526733,\n",
       " 1.5512349605560303,\n",
       " 1.3446950912475586,\n",
       " 1.1752970218658447,\n",
       " 1.372058629989624,\n",
       " 1.294616937637329,\n",
       " 1.3173682689666748,\n",
       " 1.4432865381240845,\n",
       " 1.5964338779449463,\n",
       " 1.909898042678833,\n",
       " 1.232280969619751,\n",
       " 1.5837881565093994,\n",
       " 1.7200480699539185,\n",
       " 1.4062470197677612,\n",
       " 1.3160499334335327,\n",
       " 1.6978082656860352,\n",
       " 1.5261937379837036,\n",
       " 1.6633013486862183,\n",
       " 1.602024793624878,\n",
       " 1.715173602104187,\n",
       " 1.5568166971206665,\n",
       " 1.4579088687896729,\n",
       " 1.3669854402542114,\n",
       " 1.4342550039291382,\n",
       " 1.2733412981033325,\n",
       " 1.7087398767471313,\n",
       " 1.175112247467041,\n",
       " 1.8692368268966675,\n",
       " 1.6753754615783691,\n",
       " 1.49906325340271,\n",
       " 1.3309063911437988,\n",
       " 1.5679678916931152,\n",
       " 1.3828258514404297,\n",
       " 1.6727275848388672,\n",
       " 1.3553134202957153,\n",
       " 1.4781558513641357,\n",
       " 1.8760902881622314,\n",
       " 1.671746015548706,\n",
       " 1.3767695426940918,\n",
       " 1.4467816352844238,\n",
       " 1.3449734449386597,\n",
       " 1.3066033124923706,\n",
       " 1.7380069494247437,\n",
       " 1.184739112854004,\n",
       " 1.1469370126724243,\n",
       " 1.170976161956787,\n",
       " 1.5618993043899536,\n",
       " 1.2433863878250122,\n",
       " 1.3268771171569824,\n",
       " 1.390947699546814,\n",
       " 1.885661005973816,\n",
       " 1.1481903791427612,\n",
       " 1.392335057258606,\n",
       " 1.197186827659607,\n",
       " 1.3588552474975586,\n",
       " 1.4715931415557861,\n",
       " 1.6322736740112305,\n",
       " 1.3975051641464233,\n",
       " 1.1783143281936646,\n",
       " 1.6197633743286133,\n",
       " 1.6062285900115967,\n",
       " 1.3187264204025269,\n",
       " 1.1849818229675293,\n",
       " 1.4388554096221924,\n",
       " 1.2043427228927612,\n",
       " 1.6916102170944214,\n",
       " 1.6772478818893433,\n",
       " 1.0346919298171997,\n",
       " 1.3185940980911255,\n",
       " 1.630483865737915,\n",
       " 1.6603984832763672,\n",
       " 1.6482380628585815,\n",
       " 1.4081209897994995,\n",
       " 1.198067307472229,\n",
       " 1.8636430501937866,\n",
       " 1.603672742843628,\n",
       " 1.3612068891525269,\n",
       " 1.682595133781433,\n",
       " 1.7619539499282837,\n",
       " 1.3061317205429077,\n",
       " 1.3533867597579956,\n",
       " 1.432915449142456,\n",
       " 1.4367536306381226,\n",
       " 1.6716364622116089,\n",
       " 1.4394710063934326,\n",
       " 1.027592658996582,\n",
       " 1.245218276977539,\n",
       " 1.3146791458129883,\n",
       " 1.6490731239318848,\n",
       " 1.2601699829101562,\n",
       " 1.3185172080993652,\n",
       " 1.5938935279846191,\n",
       " 1.7494410276412964,\n",
       " 1.733687400817871,\n",
       " 1.6500052213668823,\n",
       " 1.4715676307678223,\n",
       " 1.6736633777618408,\n",
       " 1.3917721509933472,\n",
       " 1.5135623216629028,\n",
       " 1.5139858722686768,\n",
       " 1.5149667263031006,\n",
       " 1.1351348161697388,\n",
       " 1.140815258026123,\n",
       " 1.6117388010025024,\n",
       " 1.6299940347671509,\n",
       " 1.2928146123886108,\n",
       " 1.176657795906067,\n",
       " 1.3278659582138062,\n",
       " 2.1116251945495605,\n",
       " 1.481512427330017,\n",
       " 1.4503378868103027,\n",
       " 1.5490200519561768,\n",
       " 1.6878340244293213,\n",
       " 1.5250508785247803,\n",
       " 1.580788493156433,\n",
       " 1.148910641670227,\n",
       " 1.663184642791748,\n",
       " 1.3038359880447388,\n",
       " 1.3587437868118286,\n",
       " 1.1433409452438354,\n",
       " 1.3469985723495483,\n",
       " 1.3198037147521973,\n",
       " 1.2543103694915771,\n",
       " 1.4527004957199097,\n",
       " 1.3824716806411743,\n",
       " 1.3050241470336914,\n",
       " 1.49510657787323,\n",
       " 1.2213571071624756,\n",
       " 1.5786954164505005,\n",
       " 1.3754252195358276,\n",
       " 1.3435871601104736,\n",
       " 1.3526089191436768,\n",
       " 1.185397982597351,\n",
       " 1.479843020439148,\n",
       " 1.3320099115371704,\n",
       " 1.5303068161010742,\n",
       " 1.5485823154449463,\n",
       " 1.3261969089508057,\n",
       " 1.6074011325836182,\n",
       " 1.511072039604187,\n",
       " 1.2316269874572754,\n",
       " 1.3095678091049194,\n",
       " 1.563104510307312,\n",
       " 1.680437445640564,\n",
       " 1.2171863317489624,\n",
       " 1.36739182472229,\n",
       " 1.2058802843093872,\n",
       " 1.6717551946640015,\n",
       " 1.6023544073104858,\n",
       " 1.2119622230529785,\n",
       " 1.2437021732330322,\n",
       " 1.3375134468078613,\n",
       " 1.3701313734054565,\n",
       " 1.499814748764038,\n",
       " 1.4384998083114624,\n",
       " 1.1711546182632446,\n",
       " 1.6334019899368286,\n",
       " 1.765622854232788,\n",
       " 1.361764669418335,\n",
       " 1.4736883640289307,\n",
       " 1.1294636726379395,\n",
       " 1.502274990081787,\n",
       " 1.5146570205688477,\n",
       " 1.2860907316207886,\n",
       " 1.3408077955245972,\n",
       " 1.144281268119812,\n",
       " 1.3909786939620972,\n",
       " 1.4786604642868042,\n",
       " 1.185643196105957,\n",
       " 1.4217815399169922,\n",
       " 1.1495375633239746,\n",
       " 1.5091938972473145,\n",
       " 1.6056714057922363,\n",
       " 1.3209415674209595,\n",
       " 1.685682773590088,\n",
       " 1.2954295873641968,\n",
       " 1.9987338781356812,\n",
       " 1.9774357080459595,\n",
       " 1.1765828132629395,\n",
       " 1.2582026720046997,\n",
       " 1.5724564790725708,\n",
       " 1.5109366178512573,\n",
       " 1.2674471139907837,\n",
       " 1.318427324295044,\n",
       " 1.3627488613128662,\n",
       " 1.3053507804870605,\n",
       " 1.525978684425354,\n",
       " 1.3915019035339355,\n",
       " 1.193568229675293,\n",
       " 1.6975703239440918,\n",
       " 1.2499722242355347,\n",
       " 1.4171630144119263,\n",
       " 1.5749545097351074,\n",
       " 1.3964555263519287,\n",
       " 1.5242393016815186,\n",
       " 1.4379128217697144,\n",
       " 1.1738226413726807,\n",
       " 1.4812556505203247,\n",
       " 1.562915563583374,\n",
       " 1.6388533115386963,\n",
       " 1.3675023317337036,\n",
       " 1.2513904571533203,\n",
       " 1.5393602848052979,\n",
       " 1.3409392833709717,\n",
       " 1.9483551979064941,\n",
       " 1.33376145362854,\n",
       " 1.7118464708328247,\n",
       " 1.460952639579773,\n",
       " 1.483293056488037,\n",
       " 1.2878752946853638,\n",
       " 1.3831223249435425,\n",
       " 1.3315658569335938,\n",
       " 1.3919934034347534,\n",
       " 1.3300827741622925,\n",
       " 1.4666666984558105,\n",
       " 1.6325151920318604,\n",
       " 1.3623281717300415,\n",
       " 1.269040584564209,\n",
       " 1.57674241065979,\n",
       " 1.8984416723251343,\n",
       " 1.076381802558899,\n",
       " 1.5356556177139282,\n",
       " 1.0535131692886353,\n",
       " 1.6746333837509155,\n",
       " 1.3835163116455078,\n",
       " 1.7965461015701294,\n",
       " 1.4020318984985352,\n",
       " 1.7783740758895874,\n",
       " 1.2854729890823364,\n",
       " 1.176721215248108,\n",
       " 1.5642634630203247,\n",
       " 1.497114896774292,\n",
       " 1.7463312149047852,\n",
       " 1.583893895149231,\n",
       " 1.4135138988494873,\n",
       " 1.4484355449676514,\n",
       " 1.410535454750061,\n",
       " 1.0823408365249634,\n",
       " 1.2016737461090088,\n",
       " 1.479731559753418,\n",
       " 1.3088881969451904,\n",
       " 1.5638586282730103,\n",
       " 1.6034172773361206,\n",
       " 1.2457619905471802,\n",
       " 1.5384150743484497,\n",
       " 1.3191349506378174,\n",
       " 1.6164002418518066,\n",
       " 1.103493571281433,\n",
       " 1.5376559495925903,\n",
       " 1.2845813035964966,\n",
       " 1.481368899345398,\n",
       " 1.7443143129348755,\n",
       " 1.4536744356155396,\n",
       " 1.4653048515319824,\n",
       " 1.3650403022766113,\n",
       " 1.491713285446167,\n",
       " 1.5557328462600708,\n",
       " 1.38906991481781,\n",
       " 1.8008577823638916,\n",
       " 1.575931191444397,\n",
       " 1.38064706325531,\n",
       " 1.1491749286651611,\n",
       " 1.1978405714035034,\n",
       " 1.3667539358139038,\n",
       " 1.2464103698730469,\n",
       " 1.4736658334732056,\n",
       " 1.1981176137924194,\n",
       " 1.3109569549560547,\n",
       " 1.364037036895752,\n",
       " 1.4308383464813232,\n",
       " 1.1917827129364014,\n",
       " 1.404388427734375,\n",
       " 1.6334209442138672,\n",
       " 1.3470381498336792,\n",
       " 1.4114351272583008,\n",
       " 1.5503896474838257,\n",
       " 1.3892253637313843,\n",
       " 1.3407301902770996,\n",
       " 1.2422677278518677,\n",
       " 1.3036690950393677,\n",
       " 1.0422601699829102,\n",
       " 1.3475946187973022,\n",
       " 1.276310920715332,\n",
       " 1.7725117206573486,\n",
       " 1.5706101655960083,\n",
       " 1.4208756685256958,\n",
       " 1.2981905937194824,\n",
       " 1.3212628364562988,\n",
       " 1.4231064319610596,\n",
       " 1.0539984703063965,\n",
       " 1.128566861152649,\n",
       " 1.3841582536697388,\n",
       " 1.5341432094573975,\n",
       " 1.3795307874679565,\n",
       " 1.328083872795105,\n",
       " 1.355987310409546,\n",
       " 1.4017781019210815,\n",
       " 1.4345145225524902,\n",
       " 1.323020577430725,\n",
       " 1.1935573816299438,\n",
       " 1.4815032482147217,\n",
       " 1.0471078157424927,\n",
       " 1.265885591506958,\n",
       " 1.5845428705215454,\n",
       " 1.65579354763031,\n",
       " 1.3795017004013062,\n",
       " 1.114045262336731,\n",
       " 1.4172425270080566,\n",
       " 1.4833053350448608,\n",
       " 1.2734718322753906,\n",
       " 1.2634892463684082,\n",
       " 1.519310712814331,\n",
       " 1.372578501701355,\n",
       " 1.5450692176818848,\n",
       " 1.7295292615890503,\n",
       " 1.696068525314331,\n",
       " 1.5641238689422607,\n",
       " 1.6645077466964722,\n",
       " 1.62345290184021,\n",
       " 1.3007817268371582,\n",
       " 1.259315013885498,\n",
       " 1.532283067703247,\n",
       " 1.3507111072540283,\n",
       " 1.5853877067565918,\n",
       " 1.664839506149292,\n",
       " 1.4935468435287476,\n",
       " 1.4959204196929932,\n",
       " 1.2467472553253174,\n",
       " 1.51775324344635,\n",
       " 1.1383297443389893,\n",
       " 1.558908462524414,\n",
       " 1.4385629892349243,\n",
       " 1.274268627166748,\n",
       " 1.4820945262908936,\n",
       " 1.0858763456344604,\n",
       " 1.2050052881240845,\n",
       " 1.6274257898330688,\n",
       " 1.5689319372177124,\n",
       " 1.5474886894226074,\n",
       " 1.4249646663665771,\n",
       " 1.5051604509353638,\n",
       " 1.5010149478912354,\n",
       " 1.4264247417449951,\n",
       " 1.5343507528305054,\n",
       " 1.5653959512710571,\n",
       " 1.7002240419387817,\n",
       " 1.4062901735305786,\n",
       " 1.4734816551208496,\n",
       " 1.5589513778686523,\n",
       " 1.3471035957336426,\n",
       " 1.245722770690918,\n",
       " 1.567325472831726,\n",
       " 1.3437281847000122,\n",
       " 1.291069746017456,\n",
       " 1.1918811798095703,\n",
       " 1.1675235033035278,\n",
       " 1.3433892726898193,\n",
       " 1.1500880718231201,\n",
       " 1.6018502712249756,\n",
       " 1.5137864351272583,\n",
       " 1.1903562545776367,\n",
       " 1.38225257396698,\n",
       " 1.2602479457855225,\n",
       " 1.454550862312317,\n",
       " 1.4101083278656006,\n",
       " 1.5031116008758545,\n",
       " 1.458527684211731,\n",
       " 1.5200389623641968,\n",
       " 1.3182438611984253,\n",
       " 1.387161135673523,\n",
       " 1.377657413482666,\n",
       " 1.2618409395217896,\n",
       " 1.3058042526245117,\n",
       " 1.3471957445144653,\n",
       " 1.3068243265151978,\n",
       " 1.9386858940124512,\n",
       " 1.0831934213638306,\n",
       " 1.6049169301986694,\n",
       " 1.0873093605041504,\n",
       " 1.5548746585845947,\n",
       " 1.4550447463989258,\n",
       " 1.1859350204467773,\n",
       " 1.3227483034133911,\n",
       " 1.2176216840744019,\n",
       " 1.4276316165924072,\n",
       " 1.875102162361145,\n",
       " 1.4318586587905884,\n",
       " 1.1464953422546387,\n",
       " 1.4855929613113403,\n",
       " 1.6403104066848755,\n",
       " 1.5661451816558838,\n",
       " 1.2633986473083496,\n",
       " 1.6952062845230103,\n",
       " 1.5183534622192383,\n",
       " 1.3002066612243652,\n",
       " 1.6418445110321045,\n",
       " 1.2056891918182373,\n",
       " 1.5328917503356934,\n",
       " 1.3984931707382202,\n",
       " 1.5376982688903809,\n",
       " 1.2026311159133911,\n",
       " 1.9907013177871704,\n",
       " 1.5485564470291138,\n",
       " 1.487898588180542,\n",
       " 1.460317850112915,\n",
       " 1.4657766819000244,\n",
       " 1.4454734325408936,\n",
       " 1.3034570217132568,\n",
       " 1.6197559833526611,\n",
       " 1.2265504598617554,\n",
       " 1.9304856061935425,\n",
       " 1.3138846158981323,\n",
       " 1.6427589654922485,\n",
       " 1.3820594549179077,\n",
       " 1.7719273567199707,\n",
       " 1.570532202720642,\n",
       " 1.2662858963012695,\n",
       " 1.1694515943527222,\n",
       " 1.3493530750274658,\n",
       " 1.3838579654693604,\n",
       " 1.616413950920105,\n",
       " 1.4652256965637207,\n",
       " 1.2437379360198975,\n",
       " 1.4677817821502686,\n",
       " 1.338442087173462,\n",
       " 1.3004356622695923,\n",
       " 1.4688715934753418,\n",
       " 1.1693317890167236,\n",
       " 1.1791976690292358,\n",
       " 1.5545612573623657,\n",
       " 1.1893209218978882,\n",
       " 1.2074229717254639,\n",
       " 1.3817118406295776,\n",
       " 1.4612950086593628,\n",
       " 1.566860318183899,\n",
       " 1.453148603439331,\n",
       " 1.5442112684249878,\n",
       " 1.4884532690048218,\n",
       " 1.4548587799072266,\n",
       " 1.3142144680023193,\n",
       " 1.194488763809204,\n",
       " 1.6112780570983887,\n",
       " 1.4794166088104248,\n",
       " 1.3487550020217896,\n",
       " 1.3332221508026123,\n",
       " 1.584791898727417,\n",
       " 1.61346435546875,\n",
       " 1.5491764545440674,\n",
       " 1.4174432754516602,\n",
       " 1.4832547903060913,\n",
       " 1.3188937902450562,\n",
       " 1.2336314916610718,\n",
       " 2.032353162765503,\n",
       " 1.5888463258743286,\n",
       " 1.5912199020385742,\n",
       " 1.3072105646133423,\n",
       " 1.4587043523788452,\n",
       " 1.5327657461166382,\n",
       " 1.4952970743179321,\n",
       " 1.3965455293655396,\n",
       " 1.4460949897766113,\n",
       " 1.4093745946884155,\n",
       " 1.5130195617675781,\n",
       " 1.4204269647598267,\n",
       " 1.3525797128677368,\n",
       " 1.7244127988815308,\n",
       " 1.5443623065948486,\n",
       " 1.2901275157928467,\n",
       " 1.3485523462295532,\n",
       " 1.5364423990249634,\n",
       " 1.7612189054489136,\n",
       " 1.3147867918014526,\n",
       " 1.5251692533493042,\n",
       " 1.663622260093689,\n",
       " 1.26734459400177,\n",
       " 1.5353997945785522,\n",
       " 1.433679461479187,\n",
       " 1.3494915962219238,\n",
       " 2.0399715900421143,\n",
       " 1.2644010782241821,\n",
       " 1.5450670719146729,\n",
       " 1.5635238885879517,\n",
       " 1.6821471452713013,\n",
       " 1.37027907371521,\n",
       " 1.6915647983551025,\n",
       " 1.3483431339263916,\n",
       " 1.4354488849639893,\n",
       " 1.477547526359558,\n",
       " 1.5776219367980957,\n",
       " 1.3290249109268188,\n",
       " 1.3191264867782593,\n",
       " 1.7604799270629883,\n",
       " 1.4886538982391357,\n",
       " 1.7538397312164307,\n",
       " 1.2687371969223022,\n",
       " 1.4075562953948975,\n",
       " 1.309591293334961,\n",
       " 1.3241980075836182,\n",
       " 1.2687004804611206,\n",
       " 1.681655764579773,\n",
       " 1.4663141965866089,\n",
       " 1.8276959657669067,\n",
       " 1.3600434064865112,\n",
       " 1.3668721914291382,\n",
       " 1.2793158292770386,\n",
       " 1.2857680320739746,\n",
       " 1.4112781286239624,\n",
       " 1.2685821056365967,\n",
       " 1.9348692893981934,\n",
       " 1.2966512441635132,\n",
       " 1.8141608238220215,\n",
       " 1.5565571784973145,\n",
       " 1.3412879705429077,\n",
       " 1.6268303394317627,\n",
       " 1.4149153232574463,\n",
       " 1.2352524995803833,\n",
       " 1.3889681100845337,\n",
       " 1.505467176437378,\n",
       " 1.3861163854599,\n",
       " 1.7252836227416992,\n",
       " 1.2890998125076294,\n",
       " 1.3942569494247437,\n",
       " 1.6070208549499512,\n",
       " 1.2173573970794678,\n",
       " 1.6803618669509888,\n",
       " 1.4686671495437622,\n",
       " 1.7533782720565796,\n",
       " 1.2844072580337524,\n",
       " 1.405128836631775,\n",
       " 1.176212191581726,\n",
       " 1.428188681602478,\n",
       " 1.6729103326797485,\n",
       " 1.3674755096435547,\n",
       " 1.344300389289856,\n",
       " 1.4288825988769531,\n",
       " 1.6986287832260132,\n",
       " 1.5778707265853882,\n",
       " 1.4012300968170166,\n",
       " 1.4457563161849976,\n",
       " 1.273452639579773,\n",
       " 1.3871550559997559,\n",
       " 1.674003005027771,\n",
       " 1.7516653537750244,\n",
       " 1.1241258382797241,\n",
       " 1.5488733053207397,\n",
       " 1.5438923835754395,\n",
       " 1.4141699075698853,\n",
       " 1.5539240837097168,\n",
       " 1.5285251140594482,\n",
       " 1.0158778429031372,\n",
       " 1.4084330797195435,\n",
       " 1.6915521621704102,\n",
       " 1.5181719064712524,\n",
       " 1.3819489479064941,\n",
       " 1.2556772232055664,\n",
       " 1.3063116073608398,\n",
       " 1.2520241737365723,\n",
       " 1.5292800664901733,\n",
       " 1.388461947441101,\n",
       " 1.1885803937911987,\n",
       " 1.199688196182251,\n",
       " 1.177184820175171,\n",
       " 1.526628851890564,\n",
       " 1.5151169300079346,\n",
       " 1.3246195316314697,\n",
       " 1.2815473079681396,\n",
       " 1.538922667503357,\n",
       " 1.493971824645996,\n",
       " 1.3155767917633057,\n",
       " 1.158687710762024,\n",
       " 1.6050865650177002,\n",
       " 1.2862396240234375,\n",
       " 1.1834518909454346,\n",
       " 1.4571919441223145,\n",
       " 1.2391282320022583,\n",
       " 1.3085384368896484,\n",
       " 1.1598124504089355,\n",
       " 1.6899974346160889,\n",
       " 1.374281883239746,\n",
       " 1.776051640510559,\n",
       " 1.4705989360809326,\n",
       " 1.311745047569275,\n",
       " 1.7067028284072876,\n",
       " 1.6656274795532227,\n",
       " 1.4822362661361694,\n",
       " 1.8447837829589844,\n",
       " 1.4872440099716187,\n",
       " 1.591430425643921,\n",
       " 1.486680269241333,\n",
       " 1.4031535387039185,\n",
       " 1.1794828176498413,\n",
       " 1.3467974662780762,\n",
       " 1.1908257007598877,\n",
       " 1.367553472518921,\n",
       " 1.1054973602294922,\n",
       " 1.1765958070755005,\n",
       " 1.4053410291671753,\n",
       " 1.433002233505249,\n",
       " 1.173506259918213,\n",
       " 1.2843586206436157,\n",
       " 1.32499361038208,\n",
       " 1.3498932123184204,\n",
       " 1.3850831985473633,\n",
       " 1.5529227256774902,\n",
       " 1.5944610834121704,\n",
       " 1.647834300994873,\n",
       " 1.341736078262329,\n",
       " 1.154868245124817,\n",
       " 1.2350677251815796,\n",
       " 1.3342074155807495,\n",
       " 1.2879198789596558,\n",
       " 1.1837166547775269,\n",
       " 1.6429920196533203,\n",
       " 1.2790946960449219,\n",
       " 1.3738874197006226,\n",
       " 1.4253333806991577,\n",
       " 1.333064317703247,\n",
       " 1.4219269752502441,\n",
       " 1.4222782850265503,\n",
       " 1.3347183465957642,\n",
       " 1.4003359079360962,\n",
       " 1.3178142309188843,\n",
       " 1.6363717317581177,\n",
       " 1.6692898273468018,\n",
       " 1.5603162050247192,\n",
       " 1.6869635581970215,\n",
       " 1.2172589302062988,\n",
       " 1.386290431022644,\n",
       " 1.5511598587036133,\n",
       " 1.303559422492981,\n",
       " 1.7174577713012695,\n",
       " 1.5592067241668701,\n",
       " 1.2954810857772827,\n",
       " 1.0178276300430298,\n",
       " 1.5447455644607544,\n",
       " 1.6284096240997314,\n",
       " 1.2156065702438354,\n",
       " 1.4915359020233154,\n",
       " 1.2414560317993164,\n",
       " 1.5643229484558105,\n",
       " 1.4943649768829346,\n",
       " 1.557151436805725,\n",
       " 1.73003351688385,\n",
       " 1.5856869220733643,\n",
       " 1.8282119035720825,\n",
       " 1.4409594535827637,\n",
       " 1.4938838481903076,\n",
       " 1.6635726690292358,\n",
       " 1.5447291135787964,\n",
       " 1.863039255142212,\n",
       " 1.3504221439361572,\n",
       " 1.263148307800293,\n",
       " 1.2395795583724976,\n",
       " 1.6368175745010376,\n",
       " 1.0841474533081055,\n",
       " 1.3422918319702148,\n",
       " 0.9922133088111877,\n",
       " 1.881230354309082,\n",
       " 1.3325691223144531,\n",
       " 1.6819050312042236,\n",
       " 1.093711018562317,\n",
       " 1.3790357112884521,\n",
       " 1.12796151638031,\n",
       " 1.2787246704101562,\n",
       " 1.2344191074371338,\n",
       " 1.2204333543777466,\n",
       " 1.2911550998687744,\n",
       " 1.425551414489746,\n",
       " 1.4835935831069946,\n",
       " 1.3822474479675293,\n",
       " 1.3200937509536743,\n",
       " 1.6482634544372559,\n",
       " 1.6983232498168945,\n",
       " 1.3404513597488403,\n",
       " 1.2769235372543335,\n",
       " 1.4798122644424438,\n",
       " 1.3528639078140259,\n",
       " 1.4740029573440552,\n",
       " 1.1821208000183105,\n",
       " 1.8655024766921997,\n",
       " 0.9844338297843933,\n",
       " 1.3581187725067139,\n",
       " 1.4120455980300903,\n",
       " 1.7213777303695679,\n",
       " 1.3763829469680786,\n",
       " 0.9788873195648193,\n",
       " 1.1921184062957764,\n",
       " 1.3583124876022339,\n",
       " 1.2576172351837158,\n",
       " 1.7177565097808838,\n",
       " 1.8919484615325928,\n",
       " 1.6539175510406494,\n",
       " 1.2810592651367188,\n",
       " 1.3295011520385742,\n",
       " 1.8065589666366577,\n",
       " 1.3637259006500244,\n",
       " 1.4032011032104492,\n",
       " 1.3283939361572266,\n",
       " 1.6311975717544556,\n",
       " 1.340885877609253,\n",
       " 1.3897418975830078,\n",
       " 1.4219024181365967,\n",
       " 1.1485037803649902,\n",
       " 1.8015010356903076,\n",
       " 1.4985932111740112,\n",
       " 1.552548885345459,\n",
       " 1.7237454652786255,\n",
       " 1.078906536102295,\n",
       " 1.3955484628677368,\n",
       " 1.2271816730499268,\n",
       " 0.9852664470672607,\n",
       " 1.4566694498062134,\n",
       " 1.3192689418792725,\n",
       " 1.3678582906723022,\n",
       " 1.622731328010559,\n",
       " 1.2055546045303345,\n",
       " 1.8576148748397827,\n",
       " 1.8418993949890137,\n",
       " 1.1602718830108643,\n",
       " 1.1295958757400513,\n",
       " 1.1748967170715332,\n",
       " 1.3096507787704468,\n",
       " 1.4435145854949951,\n",
       " 1.6645125150680542,\n",
       " 1.3584953546524048,\n",
       " 1.476688027381897,\n",
       " 1.4468176364898682,\n",
       " 1.3047981262207031,\n",
       " 1.5285536050796509,\n",
       " 1.3891226053237915,\n",
       " 1.108373999595642,\n",
       " 1.3510416746139526,\n",
       " 1.412053108215332,\n",
       " 1.2543848752975464,\n",
       " 1.4026424884796143,\n",
       " 1.5477324724197388,\n",
       " 1.4245835542678833,\n",
       " 1.4639053344726562,\n",
       " 0.9504072666168213,\n",
       " 1.3347504138946533,\n",
       " 1.4776240587234497,\n",
       " 1.3665549755096436,\n",
       " 1.693577766418457,\n",
       " 1.4301079511642456,\n",
       " 1.457957148551941,\n",
       " 1.2749789953231812,\n",
       " 1.425559163093567,\n",
       " 1.1456055641174316,\n",
       " 1.477573037147522,\n",
       " 1.4311494827270508,\n",
       " 1.5185837745666504,\n",
       " 1.4034804105758667,\n",
       " 1.3995040655136108,\n",
       " 1.204177975654602,\n",
       " 1.3406352996826172,\n",
       " 1.3934165239334106,\n",
       " 1.4704211950302124,\n",
       " 1.2882721424102783,\n",
       " 1.65664803981781,\n",
       " 1.5921696424484253,\n",
       " 1.2884726524353027,\n",
       " 1.684301495552063,\n",
       " 1.5224868059158325,\n",
       " 1.599531650543213,\n",
       " 1.5382498502731323,\n",
       " 1.5648144483566284,\n",
       " 1.5028061866760254,\n",
       " 1.0414073467254639,\n",
       " 1.3227204084396362,\n",
       " 1.5150282382965088,\n",
       " 1.1397815942764282,\n",
       " 1.3429737091064453,\n",
       " 1.4923081398010254,\n",
       " 1.9177298545837402,\n",
       " 1.4860690832138062,\n",
       " 1.9125906229019165,\n",
       " 1.4222935438156128,\n",
       " 1.2467029094696045,\n",
       " 1.269078016281128,\n",
       " 1.572096586227417,\n",
       " 1.7149658203125,\n",
       " 1.641548752784729,\n",
       " 1.4263752698898315,\n",
       " 1.449554443359375,\n",
       " 1.3765678405761719,\n",
       " 1.5179643630981445,\n",
       " 1.500968337059021,\n",
       " 1.4893285036087036,\n",
       " 1.6273095607757568,\n",
       " 1.1391226053237915,\n",
       " 1.6639680862426758,\n",
       " 1.466372013092041,\n",
       " 1.5395461320877075,\n",
       " 1.4962201118469238,\n",
       " 1.1960233449935913,\n",
       " 1.498621940612793,\n",
       " 1.524461030960083,\n",
       " 1.3850337266921997,\n",
       " 1.1552573442459106,\n",
       " 1.1512887477874756,\n",
       " 1.304023265838623,\n",
       " 1.2236336469650269,\n",
       " 1.5405571460723877,\n",
       " 1.7001758813858032,\n",
       " 1.178926944732666,\n",
       " 1.268983006477356,\n",
       " 1.4954190254211426,\n",
       " 1.3631870746612549,\n",
       " 1.7185519933700562,\n",
       " 1.2608957290649414,\n",
       " 1.3789187669754028,\n",
       " 1.581267237663269,\n",
       " 1.7574985027313232,\n",
       " 1.4741523265838623,\n",
       " 1.353420615196228,\n",
       " 1.6740211248397827,\n",
       " 1.5536361932754517,\n",
       " 1.721958875656128,\n",
       " 1.3418618440628052,\n",
       " 1.4137178659439087,\n",
       " 1.0682621002197266,\n",
       " 1.3015223741531372,\n",
       " 1.1249068975448608,\n",
       " 1.3496135473251343,\n",
       " 1.3062585592269897,\n",
       " 1.5825071334838867,\n",
       " 1.3249508142471313,\n",
       " 1.4717847108840942,\n",
       " 1.7237319946289062,\n",
       " 1.488927960395813,\n",
       " 1.5388582944869995,\n",
       " 1.6737223863601685,\n",
       " 1.5242677927017212,\n",
       " 1.5109038352966309,\n",
       " 1.2583032846450806,\n",
       " 1.1773011684417725,\n",
       " 1.5576127767562866,\n",
       " 1.6958292722702026,\n",
       " 1.34503173828125,\n",
       " 1.422504186630249,\n",
       " 1.2734795808792114,\n",
       " 1.4681769609451294,\n",
       " 1.544167160987854,\n",
       " 1.385727882385254,\n",
       " 1.610839605331421,\n",
       " 1.348746657371521,\n",
       " 1.4892348051071167,\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tloss[\"epoch_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efbfe3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.ClipCaptionModel"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824464cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (transformers)",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
